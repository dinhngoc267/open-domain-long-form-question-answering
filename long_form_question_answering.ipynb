{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Long_Form_Question_Answering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ecd77ef52544b28af7b3d68da9ab9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c61d28ec0808405fb9185d05446446a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7ef2a2b2f0d44ed808c00ad9fddccc5",
              "IPY_MODEL_dc5c7db0c6064c5cb2a67adc8bb80da0",
              "IPY_MODEL_6da91a334e7746168e0d3ed67f299674"
            ]
          }
        },
        "c61d28ec0808405fb9185d05446446a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7ef2a2b2f0d44ed808c00ad9fddccc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec158ee10d094dc4a94b100756f6db6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_956e6b0c448444a1920ce0b0f1be51c1"
          }
        },
        "dc5c7db0c6064c5cb2a67adc8bb80da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9bd69a8d05f645ddad352624e1760976",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 487,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 487,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55f94d0f7f574fb7b6149d817e7abce2"
          }
        },
        "6da91a334e7746168e0d3ed67f299674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ddd63e96bdac469683fe2a906a4b1c9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 487/487 [00:00&lt;00:00, 12.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bd7db6f3ff449dd81bd36bc560b436c"
          }
        },
        "ec158ee10d094dc4a94b100756f6db6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "956e6b0c448444a1920ce0b0f1be51c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bd69a8d05f645ddad352624e1760976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55f94d0f7f574fb7b6149d817e7abce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddd63e96bdac469683fe2a906a4b1c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bd7db6f3ff449dd81bd36bc560b436c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWCUYNOC7xSH"
      },
      "source": [
        "## Get the answer by google search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m18s6GAbk-Qa",
        "outputId": "78ca794e-7fcd-4c47-e530-8f5d5b68c5ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQiUhit_9_oJ"
      },
      "source": [
        "!pip install -qq google-search\n",
        "!pip install -qq timeout-decorator\n",
        "!pip install -qq transformers\n",
        "!pip install -qq git+https://github.com/deepset-ai/haystack.git\n",
        "!pip install -qq grpcio==1.22\n",
        "!pip install -qq gTTS\n",
        "!pip install -qq googletrans==3.1.0a0\n",
        "!pip install mutagen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwcdvwPu8aTx"
      },
      "source": [
        "from googlesearch.googlesearch import GoogleSearch\n",
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "import timeout_decorator\n",
        "from multiprocessing import Pool\n",
        "import spacy\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from tqdm import tqdm\n",
        "from haystack.retriever.dense import EmbeddingRetriever\n",
        "from haystack import Document\n",
        "from haystack.utils import print_answers, print_documents\n",
        "from haystack.pipeline import DocumentSearchPipeline\n",
        "from transformers import pipeline\n",
        "import time\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "from googletrans import Translator\n",
        "import json\n",
        "import urllib.request \n",
        "from mutagen.mp3 import MP3\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "#from sentence_transformers import util\n",
        "\n",
        "api_key = 'AIzaSyBq_8kkNb1xeBtSxPEx7Dp1J8QaII0QOuA'\n",
        "custom_search_engine_id = '44e0497b285b0dcc8'\n",
        "\n",
        "url = 'https://api.fpt.ai/hmi/tts/v5'\n",
        "\n",
        "headers = {\n",
        "    'api-key': 'EeeulwHHkFgCpw5kpj4MfwKx9JBCplQc',\n",
        "    'speed': '',\n",
        "    'voice': 'banmai'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYHQlsTf7g1H"
      },
      "source": [
        "class GoogleSearch():\n",
        "  def __init__(self, api_key, cse_id, **kwargs):\n",
        "    self.api_key = api_key\n",
        "    self.cse_id = custom_search_engine_id\n",
        "    self.args = kwargs\n",
        "\n",
        "    #discovery.build(api, version, http=http, )\n",
        "    self.service = build(\"customsearch\", \"v1\", developerKey=api_key, cache_discovery=False) \n",
        "\n",
        "  def search(self, search_term):\n",
        "      res = self.service.cse().list(q=search_term, cx=self.cse_id, **self.args).execute()\n",
        "      return res['items']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjCFVipVUVoq"
      },
      "source": [
        "class WebScraper():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def chunks(self, l, n):\n",
        "    for i in range(0, len(l), n):\n",
        "      yield l[i:i+n]\n",
        "\n",
        "  @timeout_decorator.timeout(7)\n",
        "  def scrape(self, url):\n",
        "    try:\n",
        "      html = requests.get(url, timeout=4)\n",
        "      tree = BeautifulSoup(html.text, 'lxml')\n",
        "      for invisible_elem in tree.find_all(['script','style']):\n",
        "        invisible_elem.extract()\n",
        "\n",
        "      paragraphs = [p.get_text() for p in tree.find_all('p')]\n",
        "\n",
        "      for para in tree.find_all('p'):\n",
        "        para.extract()\n",
        "\n",
        "      for href in tree.find_all(['a','strong']):\n",
        "        href.unwrap()\n",
        "      \n",
        "      tree = BeautifulSoup(str(tree.html),'lxml')\n",
        "\n",
        "      text = tree.get_text(separator='\\n\\n')\n",
        "      text = re.sub('\\n +\\n','\\n\\n',text)\n",
        "\n",
        "      paragraphs += text.split('\\n\\n')\n",
        "      paragraphs = [re.sub(' +',' ',p.strip()) for p in paragraphs]\n",
        "      paragraphs = [p for p in paragraphs if len(p.split()) > 10]\n",
        "\n",
        "      for i in range(0, len(paragraphs)):\n",
        "        sents = []\n",
        "        text_chunks = list(self.chunks(paragraphs[i],100000))\n",
        "        for chunk in text_chunks:\n",
        "          sents += sent_tokenize(chunk)\n",
        "\n",
        "        sents = [s for s in sents if len(s) > 2]\n",
        "        sents = ' '.join(sents)\n",
        "        paragraphs[i] = sents\n",
        "      return '\\n\\n'.join(paragraphs)\n",
        "    except:\n",
        "      return ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4m3cfNaoQv"
      },
      "source": [
        "class PassageParser():\n",
        "  def __init__(self, base_num_words):\n",
        "    self.base_num_words = base_num_words\n",
        "\n",
        "  def parsing(self, docs):\n",
        "    passages = []\n",
        "\n",
        "    for doc in docs:\n",
        "      sents = sent_tokenize(doc['text'])\n",
        "      text = \"\"\n",
        "\n",
        "      for sent in sents:\n",
        "        if \"©\" in sent or \"All rights reserved\" in sent:\n",
        "          break\n",
        "          \n",
        "        text += \" \" + sent\n",
        "        if len(text.split()) > self.base_num_words:\n",
        "          passage = Document(text = text.strip(), meta = {\"name\": doc['meta']['title'] or \"\"} )\n",
        "          passages.append(passage)\n",
        "          text = \"\" \n",
        "\n",
        "      if text != \"\":\n",
        "        passage = Document(text = text.strip(), meta = {\"name\": doc['meta']['title'] or \"\"})\n",
        "        passages.append(passage)\n",
        "        text = \"\"\n",
        "\n",
        "    return passages\n",
        "\n",
        "def remove_tags(text):\n",
        "  TAG_RE = re.compile(r'<[^>]+>')\n",
        "  return TAG_RE.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVRpv5MEjw35"
      },
      "source": [
        "def remove_brackets(text):\n",
        "  TAG_RE = re.compile(r'{[^}]+}')\n",
        "  return TAG_RE.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "1ecd77ef52544b28af7b3d68da9ab9c6",
            "c61d28ec0808405fb9185d05446446a6",
            "e7ef2a2b2f0d44ed808c00ad9fddccc5",
            "dc5c7db0c6064c5cb2a67adc8bb80da0",
            "6da91a334e7746168e0d3ed67f299674",
            "ec158ee10d094dc4a94b100756f6db6d",
            "956e6b0c448444a1920ce0b0f1be51c1",
            "9bd69a8d05f645ddad352624e1760976",
            "55f94d0f7f574fb7b6149d817e7abce2",
            "ddd63e96bdac469683fe2a906a4b1c9d",
            "5bd7db6f3ff449dd81bd36bc560b436c",
            "4c7d01ef3186490a90d16e591cd5fd4e",
            "39b0a79d37254aef9f9dd64db0a1ca75",
            "278b70be40f841f98f158592bb964df6",
            "f9dc926879d540eeacd8cda4859ad60f",
            "a98e8b0722a8421b9ed35c1855454cd6",
            "7b7b27f3c4164a71b5ca83e5463f4729",
            "eb8dddc6165e49d2b1c43dbeda74bbd0",
            "47218a6df8d54c189b8fd9a3a1f8fcc9"
          ]
        },
        "id": "3A4LXl4XAhsF",
        "outputId": "9521e3fd-9e2d-422f-93e1-a268bb27ed11"
      },
      "source": [
        "ggsearch = GoogleSearch(api_key, custom_search_engine_id, num=5)\n",
        "webscraper = WebScraper()\n",
        "document_store = FAISSDocumentStore(vector_dim=128, faiss_index_factory_str=\"Flat\")\n",
        "retriever = EmbeddingRetriever(document_store=document_store,\n",
        "                              embedding_model=\"yjernite/retribert-base-uncased\",\n",
        "                              model_format=\"retribert\",)\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "documentparser = PassageParser(128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ecd77ef52544b28af7b3d68da9ab9c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/487 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c7d01ef3186490a90d16e591cd5fd4e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39b0a79d37254aef9f9dd64db0a1ca75",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "278b70be40f841f98f158592bb964df6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/325M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RetriBertModel were not initialized from the model checkpoint at yjernite/retribert-base-uncased and are newly initialized: ['bert_query.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9dc926879d540eeacd8cda4859ad60f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a98e8b0722a8421b9ed35c1855454cd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b7b27f3c4164a71b5ca83e5463f4729",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb8dddc6165e49d2b1c43dbeda74bbd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47218a6df8d54c189b8fd9a3a1f8fcc9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIDkl0EVS64Y"
      },
      "source": [
        "def query(query):\n",
        "  k = 3\n",
        "\n",
        "  start = time.time()\n",
        "  results = ggsearch.search(query)\n",
        "  end = time.time()\n",
        "\n",
        "  print(\"Search is done in {} s\".format(end-start))\n",
        "\n",
        "  start = time.time()\n",
        "  docs = []\n",
        "  tmp = None\n",
        "  for result in results:\n",
        "    if \"wikipedia\" in result['link']: \n",
        "      tmp = result\n",
        "      doc = {}\n",
        "      text = webscraper.scrape(result['link'])\n",
        "      text = ' '.join([x for x in text.split() if x not in string.punctuation and x not in [\"/n\", \" \"]])\n",
        "      text = remove_brackets(text)\n",
        "      text = text.replace(\"\\n\", \"\")\n",
        "      doc = {\"text\": text, \"meta\": {\"link\": result['link'], \"name\": \"\", \"title\": remove_tags(result['htmlTitle'])}, \"embedding\": None}\n",
        "      docs.append(doc)\n",
        "  \n",
        "      results.remove(tmp)\n",
        "      k -= 1\n",
        "      break\n",
        "\n",
        "\n",
        "  for result in results:\n",
        "    doc = {}\n",
        "    text = webscraper.scrape(result['link'])\n",
        "    text = ' '.join([x for x in text.split() if x not in string.punctuation and x not in [\"/n\", \" \"]])\n",
        "    text = remove_brackets(text)\n",
        "    text = text.replace(\"\\n\", \"\")\n",
        "    doc = {\"text\": text, \"meta\": {\"link\": result['link'], \"name\": \"\", \"title\": remove_tags(result['htmlTitle'])}, \"embedding\": None}\n",
        "    docs.append(doc)\n",
        "    k -= 1\n",
        "    if k == 0:\n",
        "      break\n",
        "  end = time.time()\n",
        "\n",
        "\n",
        "  print(\"Scrapping is done in {} s\".format(end-start))\n",
        "\n",
        "  start = time.time()\n",
        "  passages = documentparser.parsing(docs)\n",
        "  end = time.time()\n",
        "  print(\"Parsing is done in {} s\".format(end-start))\n",
        "\n",
        "  start = time.time()\n",
        "  document_store.delete_documents()\n",
        "  document_store.write_documents(passages) \n",
        "  document_store.update_embeddings(retriever, batch_size=300)\n",
        "  end = time.time()\n",
        "  print(\"Embedding is done in {} s\".format(end-start))\n",
        "  start = time.time()\n",
        "  p_retrieval = DocumentSearchPipeline(retriever)\n",
        "  res = p_retrieval.run(query=query)\n",
        "  end = time.time()\n",
        "  print(\"Retrieving passages is done in {} s\".format(end-start))\n",
        "\n",
        "  text = \"\"\n",
        "  print(res)\n",
        "  for r in res['documents'][:3]:\n",
        "    text += r['text']\n",
        "  if len(text) > 4000:\n",
        "    text = text[:4000]\n",
        "  # Initialize the HuggingFace summarization pipeline\n",
        "  print('Get answering... from text long {}'.format(len(text)))\n",
        "  start = time.time()\n",
        "  summarized = summarizer(text, min_length=50, max_length=100)\n",
        "  end = time.time()\n",
        "  print(\"Get answering is done in {} s\".format(end-start))\n",
        "  summarized[0]['summary_text'] = ' '.join([x for x in summarized[0]['summary_text'].split() if x != \"\"])\n",
        "  # Print summarized text\n",
        "  return docs, passages, document_store, text, summarized[0]['summary_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2JCdYyBAFzo"
      },
      "source": [
        "def play_sound(text):\n",
        "  response = requests.request('POST', url, data=text.encode('utf-8'), headers=headers)\n",
        "  display(Audio(url=json.loads(response.text)['async'], autoplay=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7SVzeZ-BSuj"
      },
      "source": [
        "question = \"\"\n",
        "translator = Translator()\n",
        "vi_question = translator.translate(question, dest='vi').text\n",
        "\n",
        "play_sound(\"Đang tìm kiếm thông tin vui lòng đợi giây lát\")\n",
        "\n",
        "docs, passages, document_store, text, answer = query(question)\n",
        "\n",
        "print(text)\n",
        "\n",
        "# vi_answer = translator.translate(answer, dest='vi').text\n",
        "# print(answer)\n",
        "\n",
        "# play_sound(\"Đã tìm xong \"+ vi_question)\n",
        "# time.sleep(0.7)\n",
        "# play_sound(vi_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b_SnJw-Bvrv"
      },
      "source": [
        "## Train retriever model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMo073RhIy4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec94040-ab50-45b5-b0a0-c7f43b6e6309"
      },
      "source": [
        "!pip install -qq transformers\n",
        "!pip install -qq nlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 53.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiRimxQN8Mbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456,
          "referenced_widgets": [
            "8c0f5ee2682d44bdb4f1609f7bbac11f",
            "6862c659957d4594a76453a7381f8623",
            "862971b3a5694bb5b7804345eac46129",
            "b2c91c058ed649c08399e4a82ad27a42",
            "7771588cbbeb4fab8412fa15765dd7dc",
            "3051c6d2a4df468783285070e3cc9fa0",
            "90d4dc5c7f7847feb3ed79da14580c21",
            "fd82eea9a70341439b4d8605cb6d97dc",
            "52fbbb9807454f8abf1d2add3e7420aa",
            "b111fd84dec549c6ab4b921acddcfc4b",
            "d5c578d9ea3345c8b2ec0f963ac62700",
            "25a946f1919f491f8e8f21e8cb29eff2"
          ]
        },
        "outputId": "a8ed2bcd-6416-4643-d77c-a6768d2161fe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import Lambda\n",
        "import nlp\n",
        "import keras\n",
        "import string\n",
        "import math\n",
        "import random\n",
        "\n",
        "eli5 = nlp.load_dataset('eli5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c0f5ee2682d44bdb4f1609f7bbac11f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/17.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6862c659957d4594a76453a7381f8623",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.71k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset eli5/LFQA_reddit (download: 6.03 MiB, generated: 1.26 GiB, post-processed: Unknown sizetotal: 1.26 GiB) to /root/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/58e61a99404336f0891b4457a02232489b50131bdca9c1691054aeee2f6f1a6e...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "862971b3a5694bb5b7804345eac46129",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.50k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2c91c058ed649c08399e4a82ad27a42",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/576M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7771588cbbeb4fab8412fa15765dd7dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/21.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3051c6d2a4df468783285070e3cc9fa0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90d4dc5c7f7847feb3ed79da14580c21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/286M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd82eea9a70341439b4d8605cb6d97dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.65M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52fbbb9807454f8abf1d2add3e7420aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b111fd84dec549c6ab4b921acddcfc4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5c578d9ea3345c8b2ec0f963ac62700",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/18.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25a946f1919f491f8e8f21e8cb29eff2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/36.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset eli5 downloaded and prepared to /root/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/58e61a99404336f0891b4457a02232489b50131bdca9c1691054aeee2f6f1a6e. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_oLPhxG4tzW"
      },
      "source": [
        "#### Handle Eli5 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sB6kpk05Sub"
      },
      "source": [
        "> After observing: There are some answers are not relevant to the questions and there are some questions are too short (one or two words)\n",
        "\n",
        "> I just get the questions which have more than two words\n",
        "> Just get maximum 2 answers with highest score each question.\n",
        "> Just get answer which more than 6 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AzIdsHN6xPt"
      },
      "source": [
        "def is_valid_question(text):\n",
        "  if text == '[deleted by user]':\n",
        "    return False    \n",
        "  if len(text.split()) > 3:\n",
        "    return True\n",
        "  if len(text.split()) == 3:\n",
        "    if '?' in text or text[0].lower() in ['w', 'h', 'i', 'a', 'd']:\n",
        "      return True\n",
        "      \n",
        "  return False\n",
        "\n",
        "def is_valid_answer(text):\n",
        "  if len(text.split()) > 8:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.replace(\"\\n\",\"\")\n",
        "  text = ' '.join([x for x in text.split() if x != \"\" and x not in string.punctuation and \"URL\" not in x and \"@\" not in x and \"www\" not in x])\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsQbwwBiPGAt"
      },
      "source": [
        "> After cleaning and check validation\n",
        "\n",
        "- Number of all questions: 263186\n",
        "- Number of all answes: 425285\n",
        "- Number of training questions: 688469\n",
        "- Number of training answers: 688469\n",
        "\n",
        "> Too large to train. It takes 5-6 hours per epoch\n",
        "\n",
        "> I cut off to 200,000 questions by random sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpuTVBJHHINW",
        "outputId": "df3dfa02-81e5-4c94-8170-98b5d253da0f"
      },
      "source": [
        "training_data = []\n",
        "\n",
        "for idx in tqdm(range(len(eli5['train_eli5']))):\n",
        "  item = eli5['train_eli5'][idx]\n",
        "  data = {}\n",
        "  \n",
        "  data['question'] = clean_text(item['title'])\n",
        "  if is_valid_question(data['question']) == False:\n",
        "    continue\n",
        "  \n",
        "  data['answers'] = []\n",
        "\n",
        "  for ans in item['answers']['text']:\n",
        "    tmp = clean_text(ans)\n",
        "    if is_valid_answer(tmp) == True:\n",
        "      data['answers'].append(tmp)\n",
        "      if len(data['answers']) == 2:\n",
        "        break\n",
        "\n",
        "  if len(data['answers']) > 0:\n",
        "    training_data.append(data)\n",
        "\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "shuffle_indices = random.sample(range(len(training_data)), 200000)\n",
        "training_data = training_data[shuffle_indices]\n",
        "\n",
        "\n",
        "validation_data = []\n",
        "for idx in tqdm(range(len(eli5['validation_eli5']))):\n",
        "  item = eli5['validation_eli5'][idx]\n",
        "  data = {}\n",
        "  \n",
        "  data['question'] = clean_text(item['title'])\n",
        "  if is_valid_question(data['question']) == False:\n",
        "    continue\n",
        "  \n",
        "  data['answers'] = []\n",
        "\n",
        "  for ans in item['answers']['text']:\n",
        "    tmp = clean_text(ans)\n",
        "    if is_valid_answer(tmp) == True:\n",
        "      data['answers'].append(tmp)\n",
        "      if len(data['answers']) == 2:\n",
        "        break\n",
        "\n",
        "  if len(data['answers']) > 0:\n",
        "    validation_data.append(data)\n",
        "\n",
        "\n",
        "all_questions = []\n",
        "all_answers = []\n",
        "answer_question_mapping_index = []\n",
        "\n",
        "for idx in tqdm(range(len(training_data))):\n",
        "  item = training_data[idx]\n",
        "  \n",
        "  all_questions.append(item['question'])  \n",
        "  for ans in item['answers']:\n",
        "    all_answers.append(ans)\n",
        "    answer_question_mapping_index.append(idx)\n",
        "\n",
        "\n",
        "def create_pairs_dataset(data):\n",
        "  questions = []\n",
        "  answers = []\n",
        "\n",
        "  # 1: positive, 0: negative\n",
        "  labels = []\n",
        "\n",
        "  for idx in tqdm(range(len(data))):\n",
        "    item = training_data[idx]\n",
        "    for ans in item['answers']:\n",
        "      questions.append(item['question'])\n",
        "      answers.append(ans)\n",
        "      labels.append(1)\n",
        "    \n",
        "    neg_idx = np.random.randint(0, len(data),1)[0]\n",
        "    if neg_idx != idx:\n",
        "      questions.append(item['question'])\n",
        "      answers.append(data[neg_idx]['answers'][0])\n",
        "      labels.append(0)\n",
        "\n",
        "  return questions, answers, labels\n",
        "\n",
        "train_questions, train_answers, train_labels = create_pairs_dataset(training_data)\n",
        "valid_questions, valid_answers, valid_labels = create_pairs_dataset(validation_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 272634/272634 [01:17<00:00, 3535.46it/s]\n",
            "100%|██████████| 9812/9812 [00:02<00:00, 3645.56it/s]\n",
            "100%|██████████| 200000/200000 [00:00<00:00, 456130.92it/s]\n",
            "100%|██████████| 200000/200000 [00:07<00:00, 25462.87it/s]\n",
            "100%|██████████| 9572/9572 [00:00<00:00, 18835.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSSVAQQPJcfz",
        "outputId": "2ac18cbf-f70a-4f9b-a795-563fb936bf99"
      },
      "source": [
        "print('Number of all questions:',len(all_questions))\n",
        "print('Number of all answes:',len(all_answers))\n",
        "print('Number of training questions:', len(train_questions))\n",
        "print('Number of training answers:', len(train_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of all questions: 200000\n",
            "Number of all answes: 323024\n",
            "Number of training questions: 523024\n",
            "Number of training answers: 523024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHmhsgxVqNOH"
      },
      "source": [
        "train_labels = np.array(train_labels)\n",
        "valid_labels = np.array(valid_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJiOy7RaH0qf",
        "outputId": "e67d030f-1638-405e-e570-8c15adf7847d"
      },
      "source": [
        "transformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dPSiFMRDPYf"
      },
      "source": [
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "  all_tokens = []\n",
        "\n",
        "  for idx in tqdm(range(len(texts))):\n",
        "    text = texts[idx]\n",
        "\n",
        "    text = ' '.join([x for x in text.split()[:100]])\n",
        "    text = tokenizer.tokenize(text)\n",
        "    text = text[:max_len-2]\n",
        "    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "\n",
        "    pad_len = max_len - len(input_sequence)\n",
        "    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "    tokens += [0]*pad_len\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "  return np.array(all_tokens)\n",
        "\n",
        "def build_siamese_model(transformer, max_len=512, embedding_dims = 128):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "\n",
        "    #cls_token = sequence_output[:, 0, :]\n",
        "    out = tf.reduce_mean(sequence_output, axis=1)\n",
        "    out = Dense(128, activation = 'relu')(out)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INRyjWYrWj6U",
        "outputId": "0221b5a0-3dbc-4c96-fd62-c19088c4448c"
      },
      "source": [
        "train_questions_tokens = bert_encode(train_questions, tokenizer, max_len=100)\n",
        "train_answers_tokens = bert_encode(train_answers, tokenizer, max_len=100)\n",
        "\n",
        "valid_questions_tokens = bert_encode(valid_questions, tokenizer, max_len=100)\n",
        "valid_answers_tokens = bert_encode(valid_answers, tokenizer, max_len=100)\n",
        "\n",
        "all_questions_tokens = bert_encode(all_questions, tokenizer, max_len=100)\n",
        "all_answers_tokens = bert_encode(all_answers, tokenizer, max_len=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 523024/523024 [04:33<00:00, 1909.54it/s]\n",
            "100%|██████████| 523024/523024 [18:22<00:00, 474.47it/s]\n",
            "100%|██████████| 25076/25076 [00:12<00:00, 1976.01it/s]\n",
            "100%|██████████| 25076/25076 [00:51<00:00, 482.24it/s]\n",
            "100%|██████████| 200000/200000 [01:46<00:00, 1884.16it/s]\n",
            "100%|██████████| 323024/323024 [12:08<00:00, 443.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AoLA2AVj0fO"
      },
      "source": [
        "np.save(\"/content/drive/MyDrive/train_questions_tokens.npy\", train_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/train_answers_tokens.npy\", train_answers_tokens)\n",
        "np.save(\"/content/drive/MyDrive/valid_questions_tokens.npy\", valid_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/valid_answers_tokens.npy\", valid_answers_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_questions_tokens.npy\", all_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_answers_tokens.npy\", all_answers_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDi9RlMzeLq"
      },
      "source": [
        "train_questions_tokens = np.load(\"/content/drive/MyDrive/train_questions_tokens.npy\")\n",
        "train_answers_tokens = np.load(\"/content/drive/MyDrive/train_answers_tokens.npy\")\n",
        "valid_questions_tokens = np.load(\"/content/drive/MyDrive/valid_questions_tokens.npy\")\n",
        "valid_answers_tokens = np.load(\"/content/drive/MyDrive/valid_answers_tokens.npy\")\n",
        "all_questions_tokens = np.load(\"/content/drive/MyDrive/all_questions_tokens.npy\")\n",
        "all_answers_tokens = np.load(\"/content/drive/MyDrive/all_answers_tokens.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9dn0ZUPPpJU"
      },
      "source": [
        "shuffle_indices = random.sample(range(1000), 1000)\n",
        "train_answers_tokens = train_answers_tokens[shuffle_indices]\n",
        "train_questions_tokens = train_questions_tokens[shuffle_indices]\n",
        "labels = labels[shuffle_indices] \n",
        "answer_question_mapping_index = np.array(answer_question_mapping_index[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZKyHDVza5yL"
      },
      "source": [
        "class HardNegativeMiningCallback(keras.callbacks.Callback):\n",
        "  def __init__(self, all_question_tokens, all_answer_tokens, answer_question_mapping_index):\n",
        "    self.all_question_tokens = all_question_tokens\n",
        "    self.all_answer_tokens = all_answer_tokens\n",
        "    self.answer_question_mapping_index = np.array(answer_question_mapping_index)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch == 1:\n",
        "      question_embeddings = self.model.layers[2].predict(self.all_question_tokens)\n",
        "      answer_embeddings = self.model.layers[2].predict(self.all_answer_tokens)\n",
        "\n",
        "      dist_matrix = tf.reduce_sum((tf.expand_dims(answer_embeddings[:100], 1)-tf.expand_dims(question_embeddings, 0))**2,2)\n",
        "\n",
        "      for i in tqdm(range(100, answer_embeddings.shape[0], 100)):\n",
        "        tmp = tf.reduce_sum((tf.expand_dims(answer_embeddings[i:i+100], 1)-tf.expand_dims(question_embeddings, 0))**2,2)\n",
        "        dist_matrix = tf.concat([dist_matrix, tmp], axis=0)\n",
        "\n",
        "      min_dist_indices = keras.backend.eval(tf.argmin(dist_matrix, axis=1))\n",
        "\n",
        "      hard_negative_indices = (min_dist_indices != self.answer_question_mapping_index).nonzero()[0]\n",
        "      hard_negative_pairs = [[i, min_dist_indices[i]] for i in hard_negative_indices]\n",
        "      hard_negative_pairs = np.array(hard_negative_pairs)\n",
        "\n",
        "      print('\\nFound: {} hard negative pairs'.format(len(hard_negative_pairs)))\n",
        "\n",
        "      hard_negative_question_tokens = self.all_question_tokens[hard_negative_pairs[:,1]]\n",
        "      hard_negative_answer_tokens = self.all_answer_tokens[hard_negative_pairs[:,0]]\n",
        "\n",
        "      global train_questions_tokens\n",
        "      global train_answers_tokens\n",
        "      global labels\n",
        "      train_questions_tokens = np.concatenate([train_questions_tokens, hard_negative_question_tokens], axis=0)\n",
        "      train_answers_tokens = np.concatenate([train_answers_tokens, hard_negative_answer_tokens], axis=0)\n",
        "      labels = np.concatenate([labels, np.zeros(shape=(len(hard_negative_pairs)))])\n",
        "\n",
        "      shuffle_indices = random.sample(range(len(labels)), len(labels))\n",
        "      train_answers_tokens = train_answers_tokens[shuffle_indices]\n",
        "      train_questions_tokens = train_questions_tokens[shuffle_indices]\n",
        "      labels = labels[shuffle_indices] \n",
        "\n",
        "      print('Train on {} samples'.format(len(train_questions_tokens)))\n",
        "\n",
        "class DistilBertRetriever():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def euclidean_distance(self, vectors):\n",
        "    featsA, featsB = vectors\n",
        "    # compute the sum of squared distances between the vectors\n",
        "    sumSquared = K.sum(K.square(featsA - featsB), axis=1,keepdims=True)\n",
        "    # return the euclidean distance between the vectors\n",
        "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "  def contrastive_loss(self, y, preds, margin=1):\n",
        "    # explicitly cast the true class label data type to the predicted\n",
        "    # class label data type (otherwise we run the risk of having two\n",
        "    # separate data types, causing TensorFlow to error out)\n",
        "    y = tf.cast(y, preds.dtype)\n",
        "    # calculate the contrastive loss between the true labels and\n",
        "    # the predicted labels\n",
        "    squaredPreds = K.square(preds)\n",
        "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
        "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
        "    # return the computed contrastive loss to the calling function\n",
        "    return loss\n",
        "\n",
        "  def build(self):\n",
        "    A = Input(shape=100)\n",
        "    B = Input(shape=100)\n",
        "    featureExtractor = build_siamese_model(transformer_layer, max_len=100)\n",
        "    featsA = featureExtractor(A)\n",
        "    featsB = featureExtractor(B)\n",
        "    # finally, construct the siamese network\n",
        "    distance = Lambda(self.euclidean_distance)([featsA, featsB])\n",
        "    model = Model(inputs=[A, B], outputs=distance)\n",
        "    model.compile(loss=self.contrastive_loss, optimizer=Adam(learning_rate=0.0001))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufox5YQMjB6O"
      },
      "source": [
        "### Using cls token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd62jl5-B7u7"
      },
      "source": [
        "retriever = DistilBertRetriever().build()\n",
        "\n",
        "mycallback = HardNegativeMiningCallback(all_questions_tokens, all_answers_tokens, \n",
        "                                        answer_question_mapping_index[:1000])\n",
        "\n",
        "for i in range(5):\n",
        "  retriever.fit([train_questions_tokens, train_answers_tokens], train_labels,\n",
        "            validation_data=([valid_questions_tokens,valid_answers_tokens], valid_labels[:1000]),\n",
        "            batch_size=32,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks = [mycallback],\n",
        "            shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ9PK-TjEYk"
      },
      "source": [
        "### Using mean of sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIjwQk3PiaID",
        "outputId": "de9bcde4-2252-4d0a-a42f-ef298821bc22"
      },
      "source": [
        "retriever = DistilBertRetriever().build()\n",
        "\n",
        "mycallback = HardNegativeMiningCallback(all_questions_tokens, all_answers_tokens, \n",
        "                                        answer_question_mapping_index[:1000])\n",
        "\n",
        "for i in range(5):\n",
        "  retriever.fit([train_questions_tokens, train_answers_tokens], labels,\n",
        "            validation_data=([valid_questions_tokens,valid_answers_tokens], valid_labels[:1000]),\n",
        "            batch_size=32,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks = [mycallback],\n",
        "            shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 80s 2s/step - loss: 0.2832 - val_loss: 0.1751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 469.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 971 hard negative pairs\n",
            "Train on 1971 samples\n",
            "62/62 [==============================] - 104s 2s/step - loss: 0.1688 - val_loss: 0.1114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 558.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 560 hard negative pairs\n",
            "Train on 2531 samples\n",
            "80/80 [==============================] - 115s 1s/step - loss: 0.1253 - val_loss: 0.1604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 607.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 417 hard negative pairs\n",
            "Train on 2948 samples\n",
            "93/93 [==============================] - 130s 1s/step - loss: 0.0967 - val_loss: 0.1229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 520.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 286 hard negative pairs\n",
            "Train on 3234 samples\n",
            "102/102 [==============================] - 141s 1s/step - loss: 0.0704 - val_loss: 0.1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 476.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 194 hard negative pairs\n",
            "Train on 3428 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNv8ss_SqFQq"
      },
      "source": [
        "### Using 2 epochs per hard negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3zCRGleqJjx",
        "outputId": "e1f7b1f5-c5fe-43d2-e448-b8929fb9429a"
      },
      "source": [
        "retriever = DistilBertRetriever().build()\n",
        "\n",
        "mycallback = HardNegativeMiningCallback(all_questions_tokens, all_answers_tokens, \n",
        "                                        answer_question_mapping_index[:1000])\n",
        "\n",
        "for i in range(5):\n",
        "  retriever.fit([train_questions_tokens, train_answers_tokens], labels,\n",
        "            validation_data=([valid_questions_tokens,valid_answers_tokens], valid_labels[:1000]),\n",
        "            batch_size=32,\n",
        "            epochs=2,\n",
        "            verbose=1,\n",
        "            callbacks = [mycallback],\n",
        "            shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "32/32 [==============================] - 80s 2s/step - loss: 0.3465 - val_loss: 0.1824\n",
            "Epoch 2/2\n",
            "32/32 [==============================] - 58s 2s/step - loss: 0.1814 - val_loss: 0.1858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 555.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 966 hard negative pairs\n",
            "Train on 1966 samples\n",
            "Epoch 1/2\n",
            "62/62 [==============================] - 105s 2s/step - loss: 0.2537 - val_loss: 0.1911\n",
            "Epoch 2/2\n",
            "62/62 [==============================] - 94s 2s/step - loss: 0.0972 - val_loss: 0.0922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 385.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 467 hard negative pairs\n",
            "Train on 2433 samples\n",
            "Epoch 1/2\n",
            "77/77 [==============================] - 112s 1s/step - loss: 0.1109 - val_loss: 0.1094\n",
            "Epoch 2/2\n",
            "77/77 [==============================] - 111s 1s/step - loss: 0.0636 - val_loss: 0.0735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 565.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 255 hard negative pairs\n",
            "Train on 2688 samples\n",
            "Epoch 1/2\n",
            "84/84 [==============================] - 120s 1s/step - loss: 0.0647 - val_loss: 0.0917\n",
            "Epoch 2/2\n",
            "84/84 [==============================] - 120s 1s/step - loss: 0.0421 - val_loss: 0.0565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 564.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 132 hard negative pairs\n",
            "Train on 2820 samples\n",
            "Epoch 1/2\n",
            "89/89 [==============================] - 125s 1s/step - loss: 0.0345 - val_loss: 0.0399\n",
            "Epoch 2/2\n",
            "89/89 [==============================] - 125s 1s/step - loss: 0.0241 - val_loss: 0.0293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 551.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found: 111 hard negative pairs\n",
            "Train on 2931 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y0mj2QFp_gi"
      },
      "source": [
        "### Handle FQA Covid 19 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7qQToCeqJTM"
      },
      "source": [
        "import pandas as pd\n",
        "covid19_df = pd.read_csv(\"/content/drive/MyDrive/FAQ_Bank.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGMxAZ31tlwa"
      },
      "source": [
        "covid19_df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwzTaa92t4-V"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = text.replace(\"\\n\", \"\")\n",
        "  text = ' '.join([x for x in text.split() if x != \"\" and \"www\" not in x])\n",
        "\n",
        "  return text   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d0ID2jcqYHr"
      },
      "source": [
        "covid19_questions = covid19_df[covid19_df['language'] == 'en']['question'].apply(lambda x: clean_text(x))\n",
        "covid19_answers = covid19_df[covid19_df['language'] == 'en']['answer'].apply(lambda x: clean_text(x))\n",
        "covid19_questions = covid19_questions.values\n",
        "covid19_answers = covid19_answers.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5w39OJluU3o"
      },
      "source": [
        "faq_covid = pd.read_csv(\"/content/drive/MyDrive/faq_covidbert.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng0cWZ3NwVqs"
      },
      "source": [
        "faq_covid19_questions = faq_covid[faq_covid['lang']=='en']['question'].apply(lambda x: clean_text(x))\n",
        "faq_covid19_answers = faq_covid[faq_covid['lang']=='en']['answer'].apply(lambda x: clean_text(x))\n",
        "faq_covid19_questions = faq_covid19_questions.values\n",
        "faq_covid19_answers = faq_covid19_answers.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHbrmRcnxWpZ"
      },
      "source": [
        "covid19_questions = np.concatenate([covid19_questions, faq_covid19_questions])\n",
        "covid19_answers = np.concatenate([covid19_answers, faq_covid19_answers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6u3oZChah_g"
      },
      "source": [
        "covid19_train_questions = []\n",
        "covid19_train_answers = []\n",
        "covid19_train_labels = []\n",
        "\n",
        "for idx, question in enumerate(covid19_questions):\n",
        "  covid19_train_questions.append(question)\n",
        "  covid19_train_answers.append(covid19_answers[idx])\n",
        "  covid19_train_labels.append(1)\n",
        "\n",
        "  neg_indice = np.random.randint(0, len(covid19_answers), 1)[0]\n",
        "  if neg_indice != idx:\n",
        "    covid19_train_questions.append(question)\n",
        "    covid19_train_answers.append(covid19_answers[neg_indice])\n",
        "    covid19_train_labels.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcv8bX1Vy_dG",
        "outputId": "126b776b-899e-47d4-f803-73349859d658"
      },
      "source": [
        "covid19_train_questions_tokens = bert_encode(covid19_train_questions, tokenizer, max_len=100)\n",
        "covid19_train_answers_tokens = bert_encode(covid19_train_answers, tokenizer, max_len=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18721/18721 [00:17<00:00, 1072.77it/s]\n",
            "100%|██████████| 18721/18721 [00:45<00:00, 409.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGl0CCPofV6v",
        "outputId": "652d7dc4-50d9-4866-db3d-51a1656c0282"
      },
      "source": [
        "covid19_questions_tokens = bert_encode(covid19_questions, tokenizer, max_len=100)\n",
        "covid19_answers_tokens = bert_encode(covid19_answers, tokenizer, max_len=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9362/9362 [00:08<00:00, 1068.91it/s]\n",
            "100%|██████████| 9362/9362 [00:23<00:00, 402.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rSUTCddcMvP"
      },
      "source": [
        "covid19_train_labels = np.array(covid19_train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOs8g74F0QyA"
      },
      "source": [
        "shuffle_indices = random.sample(range(len(covid19_train_questions_tokens)), len(covid19_train_questions_tokens))\n",
        "covid19_train_questions_tokens = covid19_train_questions_tokens[shuffle_indices]\n",
        "covid19_train_answers_tokens = covid19_train_answers_tokens[shuffle_indices]\n",
        "covid19_train_labels = covid19_train_labels[shuffle_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2BMydWe0euy"
      },
      "source": [
        "covid19_valid_questions_tokens = covid19_train_questions_tokens[13000:]\n",
        "covid19_valid_answes_tokens = covid19_train_answers_tokens[13000:]\n",
        "covid19_valid_labels = covid19_train_labels[13000:]\n",
        "\n",
        "covid19_train_questions_tokens = covid19_train_questions_tokens[:13000]\n",
        "covid19_train_anwers_tokens = covid19_train_answers_tokens[:13000]\n",
        "covid19_train_labels = covid19_train_labels[:13000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9XMGjM2z4d4"
      },
      "source": [
        "### Combine all data and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkFfCPAB2ooA"
      },
      "source": [
        "train_questions_tokens = np.concatenate([train_questions_tokens, covid19_train_questions_tokens])\n",
        "train_answers_tokens = np.concatenate([train_answers_tokens, covid19_train_anwers_tokens])\n",
        "valid_questions_tokens = np.concatenate([valid_questions_tokens, covid19_valid_questions_tokens])\n",
        "valid_answers_tokens = np.concatenate([valid_answers_tokens, covid19_valid_answes_tokens])\n",
        "all_questions_tokens = np.concatenate([all_questions_tokens, covid19_questions_tokens])\n",
        "all_answers_tokens = np.concatenate([all_answers_tokens, covid19_answers_tokens])\n",
        "\n",
        "train_labels = np.concatenate([train_labels, covid19_train_labels])\n",
        "valid_labels = np.concatenate([valid_labels, covid19_valid_labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "385aIZlNghQt"
      },
      "source": [
        "np.save(\"/content/drive/MyDrive/all_train_questions_tokens.npy\", train_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_train_answers_tokens.npy\", train_answers_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_valid_questions_tokens.npy\", valid_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_valid_answers_tokens.npy\", valid_answers_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_all_questions_tokens.npy\", all_questions_tokens)\n",
        "np.save(\"/content/drive/MyDrive/all_all_answers_tokens.npy\", all_answers_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7i7kkxDh1mT"
      },
      "source": [
        "answer_question_mapping_index = np.array(answer_question_mapping_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEkBbHNLh3dO"
      },
      "source": [
        "answer_question_mapping_index = np.concatenate([answer_question_mapping_index, np.arange(200000, 209362)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6yl_c7l0Gkg"
      },
      "source": [
        "n_sample = len(train_answers_tokens)\n",
        "shuffle_indices = random.sample(range(n_sample), n_sample)\n",
        "train_answers_tokens = train_answers_tokens[shuffle_indices]\n",
        "train_questions_tokens = train_questions_tokens[shuffle_indices]\n",
        "train_labels = train_labels[shuffle_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "8ftRmu922m1B",
        "outputId": "47019cc3-aa0c-456c-f2ca-744732c3194c"
      },
      "source": [
        "retriever = DistilBertRetriever().build()\n",
        "\n",
        "mycallback = HardNegativeMiningCallback(all_questions_tokens, all_answers_tokens, \n",
        "                                        answer_question_mapping_index)\n",
        "\n",
        "for i in range(5):\n",
        "  retriever.fit([train_questions_tokens, train_answers_tokens], train_labels,\n",
        "            validation_data=([valid_questions_tokens,valid_answers_tokens], valid_labels),\n",
        "            batch_size=64,\n",
        "            epochs=2,\n",
        "            verbose=1,\n",
        "            callbacks = [mycallback],\n",
        "            shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            " 724/8376 [=>............................] - ETA: 5:01:46 - loss: 0.0834"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-c235be906ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmycallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             shuffle = False)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTi7dJip3Acg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}